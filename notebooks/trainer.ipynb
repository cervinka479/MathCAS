{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b82cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path (works in Jupyter)\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94091986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from src import architecture, dataset, trainer\n",
    "from utils import experiment, logger, metrics, modules, profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ba834",
   "metadata": {},
   "source": [
    "### Neural Network Architecture Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70acbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.schema import ArchitectureConfig\n",
    "from src.architecture import NeuralNetwork\n",
    "\n",
    "arch_config = ArchitectureConfig(\n",
    "    in_size=9,\n",
    "    out_size=3,\n",
    "    hidden_layers=[64, 32],\n",
    "    activation=\"ReLU\",\n",
    "    use_dropout=False,\n",
    "    dropout=0.5,\n",
    "    dropout_inplace=False,\n",
    "    final_activation=None\n",
    ")\n",
    "\n",
    "model = NeuralNetwork(arch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe09777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_size': 9,\n",
      " 'out_size': 3,\n",
      " 'hidden_layers': [64, 32],\n",
      " 'activation': 'ReLU',\n",
      " 'use_dropout': False,\n",
      " 'dropout': 0.5,\n",
      " 'dropout_inplace': False,\n",
      " 'final_activation': None}\n",
      "\n",
      "NeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Number of trainable parameters: 2819\n"
     ]
    }
   ],
   "source": [
    "# Print current model architecture\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(arch_config.model_dump(), sort_dicts=False)\n",
    "print()\n",
    "print(model)\n",
    "print()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d610b9",
   "metadata": {},
   "source": [
    "### Data Loading Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16405fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.schema import DataConfig\n",
    "from src.dataset import prepare_dataloaders\n",
    "\n",
    "data_config = DataConfig(\n",
    "    path_to_data=\"c:/Users/cervinka/cervinka/dataset_compressible_flow_60M_training_nstep180.csv\",\n",
    "    num_samples=5000,\n",
    "    batch_size=50,\n",
    "    in_cols=[\"A11\", \"A21\", \"A31\", \"A12\", \"A22\", \"A32\", \"A13\", \"A23\", \"A33\"],\n",
    "    out_cols=[\"Shear\"],\n",
    "    val_split=0.1,\n",
    "    shuffle=False,\n",
    "    sliding_window=None\n",
    ")\n",
    "\n",
    "train_loader, val_loader = prepare_dataloaders(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a1665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path_to_data': 'c:/Users/cervinka/cervinka/dataset_compressible_flow_60M_training_nstep180.csv',\n",
      " 'num_samples': 5000,\n",
      " 'batch_size': 50,\n",
      " 'in_cols': ['A11', 'A21', 'A31', 'A12', 'A22', 'A32', 'A13', 'A23', 'A33'],\n",
      " 'out_cols': ['Shear'],\n",
      " 'val_split': 0.1,\n",
      " 'shuffle': False,\n",
      " 'sliding_window': None}\n",
      "\n",
      "Train samples: 4500\n",
      "Validation samples: 500\n",
      "Train batches: 90\n",
      "Validation batches: 10\n",
      "Batch size: 50\n",
      "Example train batch X shape: torch.Size([50, 9])\n",
      "Example train batch Y shape: torch.Size([50, 1])\n",
      "Example val batch X shape: torch.Size([50, 9])\n",
      "Example val batch Y shape: torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print current data configuration\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(data_config.model_dump(), sort_dicts=False)\n",
    "print()\n",
    "def print_loader_stats(train_loader, val_loader):\n",
    "    try:\n",
    "        train_samples = len(train_loader.dataset)\n",
    "        val_samples = len(val_loader.dataset)\n",
    "    except Exception:\n",
    "        # Fallback for custom loaders\n",
    "        train_samples = sum(1 for _ in train_loader)\n",
    "        val_samples = sum(1 for _ in val_loader)\n",
    "\n",
    "    train_batches = len(train_loader)\n",
    "    val_batches = len(val_loader)\n",
    "\n",
    "    print(f\"Train samples: {train_samples}\")\n",
    "    print(f\"Validation samples: {val_samples}\")\n",
    "    print(f\"Train batches: {train_batches}\")\n",
    "    print(f\"Validation batches: {val_batches}\")\n",
    "\n",
    "    if hasattr(train_loader, 'batch_size'):\n",
    "        print(f\"Batch size: {train_loader.batch_size}\")\n",
    "\n",
    "    # Example batch shapes\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        print(f\"Example train batch X shape: {batch_x.shape}\")\n",
    "        print(f\"Example train batch Y shape: {batch_y.shape}\")\n",
    "        break\n",
    "    for batch_x, batch_y in val_loader:\n",
    "        print(f\"Example val batch X shape: {batch_x.shape}\")\n",
    "        print(f\"Example val batch Y shape: {batch_y.shape}\")\n",
    "        break\n",
    "\n",
    "print_loader_stats(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27c842",
   "metadata": {},
   "source": [
    "### Neural Network Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ae848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.schema import TrainingConfig\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"Adam\",\n",
    "    loss_function=\"MSELoss\",\n",
    "    epochs=30,\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    scheduler=\"ReduceLROnPlateau\",\n",
    "    scheduler_patience=3,\n",
    "    scheduler_factor=0.5,\n",
    "    scheduler_threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1d76f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001,\n",
      " 'optimizer': 'Adam',\n",
      " 'loss_function': 'MSELoss',\n",
      " 'epochs': 30,\n",
      " 'early_stopping': True,\n",
      " 'patience': 10,\n",
      " 'scheduler': 'ReduceLROnPlateau',\n",
      " 'scheduler_patience': 3,\n",
      " 'scheduler_factor': 0.5,\n",
      " 'scheduler_threshold': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Print current training configuration\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(training_config.model_dump(), sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e3d63",
   "metadata": {},
   "source": [
    "# Creating new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57123f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name and seed\n",
    "experiment_name = \"exp1\"\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c8c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 14:14:36,178 | INFO | Experiment directory: c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\notebooks\\outputs\\2025-08-19_14-14-36_exp1\n",
      "2025-08-19 14:14:36,178 | INFO | Seed: 42\n",
      "2025-08-19 14:14:36,180 | INFO | Config hash: 549e7dd113d2d29099a8369ad1613133\n",
      "2025-08-19 14:14:36,180 | INFO | CUDA available: True\n",
      "2025-08-19 14:14:36,181 | INFO | CUDA device: NVIDIA GeForce RTX 3070\n",
      "2025-08-19 14:14:36,182 | INFO | CUDA version: 12.1\n",
      "2025-08-19 14:14:36,182 | INFO | Using device: cuda\n",
      "2025-08-19 14:14:36,183 | INFO | Total trainable parameters: 2819\n",
      "2025-08-19 14:14:36,183 | INFO | Experiment: exp1\n",
      "2025-08-19 14:14:36,184 | INFO | Model architecture:\n",
      "NeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-08-19 14:14:36,185 | INFO | Training config: learning_rate=0.001 optimizer='Adam' loss_function='MSELoss' epochs=30 early_stopping=True patience=10 scheduler='ReduceLROnPlateau' scheduler_patience=3 scheduler_factor=0.5 scheduler_threshold=0.01\n",
      "2025-08-19 14:14:36,185 | INFO | Data config: path_to_data='c:/Users/cervinka/cervinka/dataset_compressible_flow_60M_training_nstep180.csv' num_samples=5000 batch_size=50 in_cols=['A11', 'A21', 'A31', 'A12', 'A22', 'A32', 'A13', 'A23', 'A33'] out_cols=['Shear'] val_split=0.1 shuffle=False sliding_window=None\n",
      "2025-08-19 14:14:36,186 | INFO | Training samples: 4500 | Validation samples: 500 | Batch size: 50\n"
     ]
    }
   ],
   "source": [
    "# Create new jupyter experiment\n",
    "\n",
    "import random\n",
    "import hashlib\n",
    "from utils.experiment import create_experiment_dir\n",
    "from utils.logger import setup_logger\n",
    "\n",
    "# Set output directory for experiments (relative to notebooks/outputs)\n",
    "output_dir = Path.cwd() / \"outputs\"\n",
    "exp_dir = create_experiment_dir(str(output_dir), experiment_name)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --- Logging setup ---\n",
    "log_file = os.path.join(exp_dir, \"notebook.log\")\n",
    "logger = setup_logger(verbose=True, log_to_file=True, log_file=log_file)\n",
    "\n",
    "logger.info(f\"Experiment directory: {exp_dir}\")\n",
    "logger.info(f\"Seed: {seed}\")\n",
    "\n",
    "# --- Logging setup ---\n",
    "log_file = os.path.join(exp_dir, \"notebook.log\")\n",
    "logger = setup_logger(verbose=True, log_to_file=True, log_file=log_file)\n",
    "\n",
    "# --- Config hash for reproducibility ---\n",
    "import json\n",
    "def config_hash(*configs):\n",
    "    # Combine all config dicts and hash them\n",
    "    config_str = json.dumps([c.model_dump() for c in configs], sort_keys=True)\n",
    "    return hashlib.md5(config_str.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "hash_val = config_hash(arch_config, training_config, data_config)\n",
    "logger.info(f\"Config hash: {hash_val}\")\n",
    "\n",
    "# --- CUDA/Device info ---\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    cuda_version = getattr(torch.version, \"cuda\", \"N/A\")  # type: ignore\n",
    "    logger.info(f\"CUDA version: {cuda_version}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model parameter count ---\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Total trainable parameters: {num_params}\")\n",
    "\n",
    "# --- Log experiment and configs ---\n",
    "logger.info(f\"Experiment: {experiment_name}\")\n",
    "logger.info(f\"Model architecture:\\n{model}\")\n",
    "logger.info(f\"Training config: {training_config}\")\n",
    "logger.info(f\"Data config: {data_config}\")\n",
    "\n",
    "# --- Log dataset stats ---\n",
    "try:\n",
    "    train_samples = len(train_loader.dataset) # type: ignore\n",
    "    val_samples = len(val_loader.dataset) # type: ignore\n",
    "except Exception:\n",
    "    train_samples = sum(1 for _ in train_loader)\n",
    "    val_samples = sum(1 for _ in val_loader)\n",
    "logger.info(f\"Training samples: {train_samples} | Validation samples: {val_samples} | Batch size: {data_config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838df967",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:14:42,215 | INFO | Epoch 1/30 | Train Loss: 8.5302e-01 | Val Loss: 1.2368e-01 | LR: 1.00e-03 | Time: 0:00:05\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:14:47,230 | INFO | Epoch 2/30 | Train Loss: 1.2306e-01 | Val Loss: 9.7242e-02 | LR: 1.00e-03 | Time: 0:00:10\n",
      "2025-08-19 14:14:47,231 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:14:52,251 | INFO | Epoch 3/30 | Train Loss: 9.7776e-02 | Val Loss: 8.4598e-02 | LR: 1.00e-03 | Time: 0:00:15\n",
      "2025-08-19 14:14:52,252 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:14:57,333 | INFO | Epoch 4/30 | Train Loss: 8.4708e-02 | Val Loss: 7.8318e-02 | LR: 1.00e-03 | Time: 0:00:20\n",
      "2025-08-19 14:14:57,334 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:02,320 | INFO | Epoch 5/30 | Train Loss: 7.9828e-02 | Val Loss: 7.6236e-02 | LR: 1.00e-03 | Time: 0:00:25\n",
      "2025-08-19 14:15:02,321 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:07,298 | INFO | Epoch 6/30 | Train Loss: 7.6562e-02 | Val Loss: 7.3432e-02 | LR: 1.00e-03 | Time: 0:00:30\n",
      "2025-08-19 14:15:07,299 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:12,272 | INFO | Epoch 7/30 | Train Loss: 7.4542e-02 | Val Loss: 7.2815e-02 | LR: 1.00e-03 | Time: 0:00:35\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:17,255 | INFO | Epoch 8/30 | Train Loss: 6.9859e-02 | Val Loss: 6.4161e-02 | LR: 1.00e-03 | Time: 0:00:40\n",
      "2025-08-19 14:15:17,256 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:22,247 | INFO | Epoch 9/30 | Train Loss: 5.9366e-02 | Val Loss: 5.2076e-02 | LR: 1.00e-03 | Time: 0:00:45\n",
      "2025-08-19 14:15:22,248 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:27,259 | INFO | Epoch 10/30 | Train Loss: 4.9117e-02 | Val Loss: 4.2936e-02 | LR: 1.00e-03 | Time: 0:00:50\n",
      "2025-08-19 14:15:27,260 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:32,250 | INFO | Epoch 11/30 | Train Loss: 3.8196e-02 | Val Loss: 3.1467e-02 | LR: 1.00e-03 | Time: 0:00:55\n",
      "2025-08-19 14:15:32,251 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:37,280 | INFO | Epoch 12/30 | Train Loss: 2.6367e-02 | Val Loss: 2.1239e-02 | LR: 1.00e-03 | Time: 0:01:00\n",
      "2025-08-19 14:15:37,280 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:42,267 | INFO | Epoch 13/30 | Train Loss: 1.7336e-02 | Val Loss: 1.3550e-02 | LR: 1.00e-03 | Time: 0:01:05\n",
      "2025-08-19 14:15:42,268 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:47,329 | INFO | Epoch 14/30 | Train Loss: 1.1216e-02 | Val Loss: 9.2775e-03 | LR: 1.00e-03 | Time: 0:01:10\n",
      "2025-08-19 14:15:47,329 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:52,327 | INFO | Epoch 15/30 | Train Loss: 7.5125e-03 | Val Loss: 7.0094e-03 | LR: 1.00e-03 | Time: 0:01:15\n",
      "2025-08-19 14:15:52,327 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:15:57,313 | INFO | Epoch 16/30 | Train Loss: 5.6211e-03 | Val Loss: 6.0653e-03 | LR: 1.00e-03 | Time: 0:01:20\n",
      "2025-08-19 14:15:57,313 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:02,353 | INFO | Epoch 17/30 | Train Loss: 4.6739e-03 | Val Loss: 6.0604e-03 | LR: 1.00e-03 | Time: 0:01:25\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:07,338 | INFO | Epoch 18/30 | Train Loss: 4.1649e-03 | Val Loss: 5.6797e-03 | LR: 1.00e-03 | Time: 0:01:30\n",
      "2025-08-19 14:16:07,339 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:12,340 | INFO | Epoch 19/30 | Train Loss: 3.8796e-03 | Val Loss: 4.8606e-03 | LR: 1.00e-03 | Time: 0:01:35\n",
      "2025-08-19 14:16:12,341 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:17,345 | INFO | Epoch 20/30 | Train Loss: 3.6772e-03 | Val Loss: 4.7288e-03 | LR: 1.00e-03 | Time: 0:01:40\n",
      "2025-08-19 14:16:17,346 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:22,315 | INFO | Epoch 21/30 | Train Loss: 3.4761e-03 | Val Loss: 4.5610e-03 | LR: 1.00e-03 | Time: 0:01:45\n",
      "2025-08-19 14:16:22,315 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:27,267 | INFO | Epoch 22/30 | Train Loss: 3.3572e-03 | Val Loss: 4.7518e-03 | LR: 1.00e-03 | Time: 0:01:50\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:32,253 | INFO | Epoch 23/30 | Train Loss: 3.2629e-03 | Val Loss: 4.3954e-03 | LR: 1.00e-03 | Time: 0:01:55\n",
      "2025-08-19 14:16:32,254 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:37,221 | INFO | Epoch 24/30 | Train Loss: 3.1542e-03 | Val Loss: 4.3146e-03 | LR: 1.00e-03 | Time: 0:02:00\n",
      "2025-08-19 14:16:37,222 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:42,186 | INFO | Epoch 25/30 | Train Loss: 3.0427e-03 | Val Loss: 4.3354e-03 | LR: 1.00e-03 | Time: 0:02:05\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:47,179 | INFO | Epoch 26/30 | Train Loss: 3.0423e-03 | Val Loss: 4.3419e-03 | LR: 1.00e-03 | Time: 0:02:10\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:52,193 | INFO | Epoch 27/30 | Train Loss: 2.9315e-03 | Val Loss: 4.0183e-03 | LR: 1.00e-03 | Time: 0:02:15\n",
      "2025-08-19 14:16:52,194 | INFO | Learning rate reduced to 1.00e-03\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:16:57,206 | INFO | Epoch 28/30 | Train Loss: 2.8344e-03 | Val Loss: 4.0690e-03 | LR: 1.00e-03 | Time: 0:02:20\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:17:02,195 | INFO | Epoch 29/30 | Train Loss: 2.8410e-03 | Val Loss: 4.2472e-03 | LR: 1.00e-03 | Time: 0:02:25\n",
      "c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-19 14:17:07,209 | INFO | Epoch 30/30 | Train Loss: 2.7568e-03 | Val Loss: 4.0806e-03 | LR: 1.00e-03 | Time: 0:02:30\n",
      "2025-08-19 14:17:07,210 | INFO | Training complete.\n",
      "2025-08-19 14:17:07,211 | INFO | Best model at epoch 27 | Val Loss: 4.0183e-03\n",
      "2025-08-19 14:17:07,212 | INFO | Total training time: 0:02:30\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop #\n",
    "\n",
    "from src.trainer import train_one_epoch, validate\n",
    "from utils.modules import get_loss_function, get_optimizer, get_scheduler\n",
    "from utils.metrics import format_time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = get_loss_function(training_config.loss_function)\n",
    "optimizer = get_optimizer(\n",
    "    training_config.optimizer,\n",
    "    model.parameters(),\n",
    "    training_config.learning_rate\n",
    ")\n",
    "\n",
    "scheduler = None\n",
    "if training_config.scheduler == \"ReduceLROnPlateau\":\n",
    "    scheduler = get_scheduler(\n",
    "        \"ReduceLROnPlateau\",\n",
    "        optimizer,\n",
    "        patience=training_config.scheduler_patience or 2,\n",
    "        factor=training_config.scheduler_factor or 0.5,\n",
    "        threshold=training_config.scheduler_threshold or 1e-4,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "patience_counter = 0\n",
    "train_losses, val_losses, elapsed_times = [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(training_config.epochs):\n",
    "\n",
    "        train_loader, val_loader = prepare_dataloaders(data_config, epoch=epoch)\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_times.append(elapsed_time)\n",
    "\n",
    "        # Log epoch information\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch + 1}/{training_config.epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4e} | Val Loss: {val_loss:.4e} | \"\n",
    "            f\"LR: {current_lr:.2e} | Time: {format_time(elapsed_time)}\"\n",
    "        )\n",
    "\n",
    "        # Log scheduler events (ReduceLROnPlateau)\n",
    "        if scheduler is not None and hasattr(scheduler, 'num_bad_epochs') and scheduler.num_bad_epochs == 0 and epoch > 0:\n",
    "            logger.info(f\"Learning rate reduced to {current_lr:.2e}\")\n",
    "\n",
    "        if training_config.early_stopping and patience_counter >= training_config.patience:\n",
    "                    logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(\"Training complete.\")\n",
    "    logger.info(f\"Best model at epoch {best_epoch} | Val Loss: {best_val_loss:.4e}\")\n",
    "    logger.info(f\"Total training time: {format_time(total_time)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    logger.error(\"Exception during training:\\n\" + traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad773d2",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0530cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to c:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\notebooks\\outputs\\2025-08-19_14-14-36_exp1\\best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save best model #\n",
    "\n",
    "best_model_path = os.path.join(exp_dir, \"best_model.pt\")\n",
    "torch.save(best_model_state, best_model_path)\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history #\n",
    "\n",
    "history_path = os.path.join(exp_dir, \"training_history.pt\")\n",
    "torch.save({\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "}, history_path)\n",
    "print(f\"Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d72f2",
   "metadata": {},
   "source": [
    "### Additional training (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5960b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best model weights\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28172d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload optimizer state (if you want to preserve momentum, etc.)\n",
    "optimizer.load_state_dict(torch.load(history_path)[\"optimizer_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab3686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
