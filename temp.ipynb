{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'datasets/dataset_compressible_flow_60M_training_nstep180.csv')\n",
    "\n",
    "# Extract data from 50 million to 55 million (assuming row indices)\n",
    "test_df = df.iloc[50_000_000:55_000_000]\n",
    "\n",
    "# Save as test dataset\n",
    "test_df.to_csv('datasets/dataset_compressible_flow_5M_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_path = r'datasets\\dataset_compressible_flow_60M_training_nstep180.csv'\n",
    "\n",
    "with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    headers = next(reader)\n",
    "    print(\"Headings:\", headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a0afe",
   "metadata": {},
   "source": [
    "## Create prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff564ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.evaluation import load_model, write_results_to_file\n",
    "from config import load_config\n",
    "\n",
    "shear_exp_dir = project_root / r\"C:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\outputs\\2025-08-26_12-47-54_shear_dan_750\"\n",
    "shear_config = load_config(shear_exp_dir / \"config.yaml\")\n",
    "shear_model = load_model(shear_config, shear_exp_dir)\n",
    "shear_model.eval()\n",
    "\n",
    "res_exp_dir = project_root / r\"C:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\outputs\\2025-09-16_01-10-21_res_predShear750_dan_750\"\n",
    "res_config = load_config(res_exp_dir / \"config.yaml\")\n",
    "res_model = load_model(res_config, res_exp_dir)\n",
    "res_model.eval()\n",
    "\n",
    "chunk_size = 100_000  # Adjust as needed for your memory\n",
    "input_path = r\"datasets\\dataset_compressible_flow_5M_test.csv\"\n",
    "output_path = r\"temp_output.csv\"\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "total_inference_time = 0.0\n",
    "\n",
    "first = True\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "    # Predict shear\n",
    "    shear_inputs = chunk[shear_config.data.in_cols].values\n",
    "    shear_tensor = torch.tensor(shear_inputs, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        pred_shear_all = shear_model(shear_tensor).detach().cpu().numpy()\n",
    "    # Select only the Shear prediction (last column)\n",
    "    if pred_shear_all.ndim == 2:\n",
    "        pred_shear = pred_shear_all[:, -1]\n",
    "    else:\n",
    "        pred_shear = pred_shear_all[-1]\n",
    "    chunk[\"PredShear\"] = pred_shear\n",
    "\n",
    "    # Predict with second model and measure time\n",
    "    res_inputs = chunk[res_config.data.in_cols].values\n",
    "    res_tensor = torch.tensor(res_inputs, dtype=torch.float32)\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        res_pred = res_model(res_tensor).squeeze().numpy()\n",
    "    total_inference_time += time.time() - start_time\n",
    "\n",
    "    output_cols = res_config.data.out_cols\n",
    "    y_true = chunk[output_cols].values\n",
    "    y_pred = res_pred if res_pred.ndim == 2 else res_pred.reshape(-1, 1)\n",
    "\n",
    "    all_y_true.append(y_true)\n",
    "    all_y_pred.append(y_pred)\n",
    "\n",
    "    # # Save chunk with PredShear\n",
    "    # chunk.to_csv(output_path, mode='w' if first else 'a', header=first, index=False)\n",
    "    # first = False\n",
    "\n",
    "y_true_full = np.concatenate(all_y_true, axis=0)\n",
    "y_pred_full = np.concatenate(all_y_pred, axis=0)\n",
    "\n",
    "# Write metrics and predictions to file\n",
    "write_results_to_file(\n",
    "    config=res_config,\n",
    "    y_pred=y_pred_full,\n",
    "    y_true=y_true_full,\n",
    "    exp_dir=res_exp_dir,\n",
    "    model=res_model,\n",
    "    inference_time=total_inference_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad238c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000001\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open(r\"C:\\Users\\cervinka\\cervinka\\GitHub\\MathCAS\\datasets\\dataset_compressible_flow_eda_50M_training_nstep180_with_dan_1500.csv\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
